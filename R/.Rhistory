setwd('/Users/krishnakalyan3/Educational/PumpIt/data')
set.seed(123)
train <- read_csv('train_set_values.csv')
library(readr)
train <- read_csv('train_set_values.csv')
str(train)
glimpse(train)
library(dplyr)
train <- read_csv('train_set_values.csv')
glimpse(train)
dim(train)
summary(train)
glimpse(train)
ls
ls()
library(devtools)
options(devtools.install.args = "--no-multiarch") # if you have 64-bit R only, you can skip this
install_github("Microsoft/LightGBM", subdir = "R-package")
library(lightgbm)
install_github("Microsoft/LightGBM", subdir = "R-package")
library(lightgbm)
y_train <- read_csv('train_set_labels.csv')
glimpse(y_train)
table(y_train$status_group)
config_file <- '/Users/krishnakalyan3/R_internship/config.yaml'
conf_file <- spark_config(file = config_file)
sc <- spark_connect(master = "local", app_name = 'ddR')
sc <- spark_connect(master = "local", app_name = 'ddR')
# Install SparklyR
#install.packages("sparklyr")
library(sparklyr)
library(dplyr)
sc <- spark_connect(master = "local", app_name = 'ddR')
setClass("spark.ddR",
contains = "ddRDriver",
slots = c(sc = "spark_connection"))
setClass(Class = "spark.ddR",
slots = c(sc = "spark_connection"))
?setMethod
setMethod(f = "init", signature = "spark.ddR", definition = function(master='local'){
sc = sparklyr::spark_connect(master, app_name = "ddR", config = spark_conf)
})
setMethod(f = "init", signature = "spark.ddR",
definition = function(master='local'){
sc = sparklyr::spark_connect(master,
app_name = "ddR", config = spark_conf)
})
setMethod(f = "spark_connection", signature = "spark.ddR",
definition = function(master='local'){
sc = sparklyr::spark_connect(master,
app_name = "ddR", config = spark_conf)
})
new(Class="spark.ddR")
setMethod(f = "init", signature = "spark.ddR",
definition = function(master='local'){
sc = sparklyr::spark_connect(master,
app_name = "ddR", config = spark_conf)
})
setMethod(f = "init", signature = "spark.ddR",
definition = function(x){
cat("*** ")
cat(x)
})
setMethod(f = "inita", signature = "spark.ddR",
definition = function(x){
cat("*** ")
})
setMethod(f = "init", signature = "spark.ddR",
definition = function(x){
cat("*** ")
})
setMethod(f = "init", signature = "spark.ddR",
definition = function(x){
cat("*** ")
})
setGeneric("init")
setMethod(f = "init", signature = "spark.ddR",
definition = function(x){
cat("*** ")
})
setClass(Class = "spark.ddR",
representation(sc = "spark_connection"))
setMethod(f = "init", "spark.ddR",
definition = function(x, y){
cat("*** ")
})
setMethod("abc", "spark.ddR",
definition = function(x, y){
cat("*** ")
})
setClass(Class = "spark.ddR",
representation(sc = "spark_connection"))
setMethod("abc", "spark.ddR",
definition = function(x, y){
cat("*** ")
})
setMethod("plot", "spark.ddR",
definition = function(x, y){
cat("*** ")
})
setMethod("init", "spark.ddR",
definition = function(x, y){
cat("*** ")
})
setMethod("cbind", "spark.ddR",
definition = function(x, y){
cat("*** ")
})
setMethod("name", "spark.ddR",
definition = function(x, y){
cat("*** ")
})
setMethod("names", "spark.ddR",
definition = function(x, y){
cat("*** ")
})
setMethod("plot", "spark.ddR",
definition = function(x, y){
cat("*** ")
})
plot()
plot(a)
setClass("Person", representation(name = "character", age = "numeric"))
setClass("Shape")
setClass("Polygon", representation(sides = "integer"), contains = "Shape")
setMethod("sides", signature(object = "Polygon"), function(object) {
object@sides
})
setMethod("sides", signature(object = "Polygon"), function(object) {
object@sides
})
setClass("Polygon", representation(sides = "integer"), contains = "Shape")
setMethod("sides", signature(object = "Polygon"), function(object) {
object@sides
})
sides <- function(object) 0
setMethod("sides", signature(object = "Polygon"), function(object) {
object@sides
})
setClass("spark.ddR",
slots = c(sc = "spark_connection"))
init_spark <- function(master = "local", config_file, ...) {
message("Backend switched to Spark. Initializing the Spark Context")
if (hasArg(config_file)) {
message("Applying user specified configuration file")
spark_conf <- spark_config(file = config_file)
sc = sparklyr::spark_connect(master, app_name = "ddR", config = spark_conf)
} else{
sc = sparklyr::spark_connect(master, app_name = "ddR")
}
# http://apache-spark-user-list.1001560.n3.nabble.com/Getting-the-number-of-slaves-td10604.html
memory_status <-
sparklyr::invoke(sc$spark_context, "getExecutorMemoryStatus")
num_exec <- length(memory_status)
new(
"spark.ddR",
DListClass = "ParallelObj",
DFrameClass = "ParallelObj",
DArrayClass = "ParallelObj",
name = "spark",
executors = num_exec,
sc = sc
)
}
#' @export
setMethod("shutdown", "spark.ddR", function(x) {
message("Stopping the Spark Context")
sparklyr::spark_disconnect(sc)
})
#' @export
setGeneric("get_parts", function(x, index, ...)
standardGeneric("get_parts"))
#' @export
setGeneric("do_collect", function(x, parts)
standardGeneric("do_collect"))
#' @export
setMethod("shutdown", "spark.ddR", function(x) {
message("Stopping the Spark Context")
sparklyr::spark_disconnect(sc)
})
shutdown <- function(x){
return(0)
}
#' @export
setMethod("shutdown", "spark.ddR", function(x) {
message("Stopping the Spark Context")
sparklyr::spark_disconnect(sc)
})
#' @export
setMethod("shutdown", "spark.ddR", function(x) {
message("Stopping the Spark Context")
sparklyr::spark_disconnect(sc)
})
new('spark.ddR')
k <- new('spark.ddR')
k
spark.ddr
#' @export
setMethod("shutdown", "spark.ddR", function(x) {
message("Stopping the Spark Context")
sparklyr::spark_disconnect(sc)
})
